{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "68949ea7-77c9-4887-a575-a2a9532ea913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, Flatten, Dense, Concatenate, Multiply, Dropout, Activation, Dot\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "# Load datasets\n",
    "user_activity = pd.read_csv('../vibrent/user_activity_triplets.csv', sep=';', engine='python', encoding='utf-8', on_bad_lines='skip', dtype=str, header=0)\n",
    "outfits = pd.read_csv('../vibrent/outfits.csv', sep=';', engine='python', encoding='utf-8', on_bad_lines='skip', dtype=str, header=0)\n",
    "picture_triplets = pd.read_csv('../vibrent/picture_triplets.csv', sep=';', engine='python', encoding='utf-8', on_bad_lines='skip', dtype=str, header=0)\n",
    "\n",
    "\n",
    "## Create mappings because customer.id and outfit.id are strings and not in order.\n",
    "# Map IDs to indices\n",
    "user_id_mapping = {id: idx for idx, id in enumerate(user_activity['customer.id'].unique())}\n",
    "item_id_mapping = {id: idx for idx, id in enumerate(user_activity['outfit.id'].unique())}\n",
    "# Apply mappings\n",
    "user_activity['user_idx'] = user_activity['customer.id'].map(user_id_mapping)\n",
    "user_activity['item_idx'] = user_activity['outfit.id'].map(item_id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a9eb5b8f-1cf7-432a-9a52-60146302a8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Preprocess metadata\n",
    "\n",
    "\n",
    "## Clean descriptions\n",
    "## First makes all lowercase, then fills NA with empty string before replacing everything not a-z with empty string before removing leading and trailing whitespace\n",
    "outfits['description'] = outfits['description'].str.lower().fillna('').replace(r'[^a-z\\s]', '', regex=True).str.strip()\n",
    "\n",
    "# Initialize TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=500,  # Limit vocabulary to 500 terms\n",
    "    stop_words='english',  # Remove stopwords\n",
    "    ngram_range=(1, 2)  # Include unigrams and bigrams\n",
    ")\n",
    "\n",
    "# Generate TF-IDF matrix\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(outfits['description']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9ac0c2c5-f161-4c34-ac3f-f26346cc0a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature matrix dimensions: (15649, 1156)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess metadata\n",
    "\n",
    "# Define the cleaning function. Outfit tags have many brackets we want to remove. \n",
    "clean_str = lambda x: re.sub(r\"[\\'\\[\\]]\", \"\", x).strip()\n",
    "\n",
    "#Remove brackets and split the tags on the comma\n",
    "outfits['tags_list'] = outfits['outfit_tags'].apply(lambda x: [clean_str(tag) for tag in x.split(',')])\n",
    "\n",
    "# Create a matrix of all the unique tags and outfits.\n",
    "mlb = MultiLabelBinarizer()\n",
    "tag_features = mlb.fit_transform(outfits['tags_list'])\n",
    "\n",
    "## Outfit Group mapped to a numerical value\n",
    "group_encoder = LabelEncoder()\n",
    "outfits['group_encoded'] = group_encoder.fit_transform(outfits['group'])\n",
    "\n",
    "## Numerical Features\n",
    "numerical_features = ['retailPrice', 'pricePerWeek', 'pricePerMonth']\n",
    "# FIll NA with median values\n",
    "outfits[numerical_features] = outfits[numerical_features].apply(pd.to_numeric, errors='coerce').apply(lambda col: col.fillna(col.median()))\n",
    "\n",
    "# Scale the numerical features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_numerical_features = scaler.fit_transform(outfits[numerical_features])\n",
    "\n",
    "## Combine all item features by cocatenation\n",
    "item_features = np.concatenate([\n",
    "    tag_features,\n",
    "    outfits['group_encoded'].values.reshape(-1, 1),\n",
    "    scaled_numerical_features,\n",
    "    tfidf_features\n",
    "], axis=1)\n",
    "\n",
    "# Output the dimensionality\n",
    "print(\"Final feature matrix dimensions:\", item_features.shape)\n",
    "\n",
    "item_feature_dim = item_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "81814a5d-4fff-41d3-9baa-530d74b78481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2293/2293 [00:02<00:00, 847.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Map outfit IDs to item indices, same as we did with user_activity\n",
    "outfits['item_idx'] = outfits['id'].map(item_id_mapping)\n",
    "\n",
    "# Create item feature matrix aligned with item indices (item_idx)\n",
    "num_items = len(item_id_mapping)\n",
    "item_features_aligned = np.zeros((num_items, item_feature_dim))\n",
    "\n",
    "for idx, row in outfits.iterrows():\n",
    "    item_idx = row['item_idx']\n",
    "    if np.isnan(item_idx):\n",
    "        continue\n",
    "    item_idx = int(item_idx)\n",
    "    item_features_aligned[item_idx] = item_features[idx]\n",
    "\n",
    "# Generate negative samples, essentially negative sample = outfit user has not interacted with\n",
    "def generate_negatives(df, num_negatives):\n",
    "    user_item_set = set(zip(df['user_idx'], df['item_idx']))\n",
    "    all_items = set(df['item_idx'].unique())\n",
    "    negatives = []\n",
    "\n",
    "    for user in tqdm(df['user_idx'].unique()):\n",
    "        items_rented = set(df[df['user_idx'] == user]['item_idx'])\n",
    "        non_interacted_items = list(all_items - items_rented)\n",
    "        sampled_negatives = np.random.choice(\n",
    "            non_interacted_items, size=num_negatives, replace=True\n",
    "        )\n",
    "        negatives.extend([(user, item, 0) for item in sampled_negatives]) # 0 is used in testing and validation\n",
    "\n",
    "    return negatives\n",
    "\n",
    "number_negative_per_positive = 5\n",
    "num_negatives = 28*number_negative_per_positive # Approximately 28 is the average number of rented items per customer.\n",
    "\n",
    "# Positive samples\n",
    "user_activity['label'] = 1 # Set all rented items to be 1 for testing and validation\n",
    "positive_samples = user_activity[['user_idx', 'item_idx', 'label']] # Use mappings and label.\n",
    "\n",
    "# Negative samples\n",
    "negative_samples = generate_negatives(user_activity, num_negatives)\n",
    "negative_df = pd.DataFrame(negative_samples, columns=['user_idx', 'item_idx', 'label'])\n",
    "\n",
    "# Combine and shuffle the dataset containing both positive and negative samples.\n",
    "data = pd.concat([positive_samples, negative_df])\n",
    "data = data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d6897bbc-ac08-478b-9e9c-b29699de9255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare training data\n",
    "train_user = train_data['user_idx'].values\n",
    "train_item = train_data['item_idx'].values\n",
    "train_label = train_data['label'].values\n",
    "train_item_metadata = item_features_aligned[train_item]\n",
    "\n",
    "# Prepare testing data\n",
    "test_user = test_data['user_idx'].values\n",
    "test_item = test_data['item_idx'].values\n",
    "test_label = test_data['label'].values\n",
    "test_item_metadata = item_features_aligned[test_item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2e3453ea-bbe2-4ca2-b9ed-02bc8c340591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_dir/neumf_tuning_v2_v8/tuner0.json\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andersho/.local/lib/python3.10/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['user_input', 'item_input', 'item_metadata_input']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1205/1205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.2103 - auc: 0.5033 - loss: 5.1081 - precision: 0.1674 - recall: 0.9335 - val_accuracy: 0.1676 - val_auc: 0.8044 - val_loss: 1.1541 - val_precision: 0.1676 - val_recall: 1.0000\n",
      "Epoch 2/25\n",
      "\u001b[1m1205/1205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.1655 - auc: 0.7312 - loss: 1.9778 - precision: 0.1655 - recall: 1.0000 - val_accuracy: 0.1676 - val_auc: 0.8171 - val_loss: 1.2708 - val_precision: 0.1676 - val_recall: 1.0000\n",
      "Epoch 3/25\n",
      "\u001b[1m1205/1205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.1673 - auc: 0.7767 - loss: 1.8926 - precision: 0.1673 - recall: 1.0000 - val_accuracy: 0.1676 - val_auc: 0.8225 - val_loss: 1.3206 - val_precision: 0.1676 - val_recall: 1.0000\n",
      "Epoch 4/25\n",
      "\u001b[1m1205/1205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.1676 - auc: 0.8224 - loss: 1.8367 - precision: 0.1676 - recall: 1.0000 - val_accuracy: 0.1676 - val_auc: 0.8273 - val_loss: 1.2853 - val_precision: 0.1676 - val_recall: 1.0000\n",
      "Epoch 5/25\n",
      "\u001b[1m1205/1205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.1709 - auc: 0.8558 - loss: 1.7333 - precision: 0.1674 - recall: 1.0000 - val_accuracy: 0.1969 - val_auc: 0.8388 - val_loss: 1.1492 - val_precision: 0.1723 - val_recall: 0.9963\n",
      "Epoch 6/25\n",
      "\u001b[1m1205/1205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.2827 - auc: 0.8900 - loss: 1.5765 - precision: 0.1892 - recall: 0.9995 - val_accuracy: 0.3966 - val_auc: 0.8412 - val_loss: 0.9202 - val_precision: 0.2130 - val_recall: 0.9646\n",
      "Epoch 7/25\n",
      "\u001b[1m1205/1205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.4870 - auc: 0.9218 - loss: 1.3897 - precision: 0.2452 - recall: 0.9981 - val_accuracy: 0.5013 - val_auc: 0.8538 - val_loss: 0.9412 - val_precision: 0.2446 - val_recall: 0.9459\n",
      "Epoch 8/25\n",
      "\u001b[1m1205/1205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.6175 - auc: 0.9443 - loss: 1.2117 - precision: 0.3042 - recall: 0.9964 - val_accuracy: 0.5781 - val_auc: 0.8546 - val_loss: 0.8816 - val_precision: 0.2743 - val_recall: 0.9218\n",
      "Epoch 9/25\n",
      "\u001b[1m1205/1205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.6939 - auc: 0.9573 - loss: 1.0676 - precision: 0.3545 - recall: 0.9960 - val_accuracy: 0.6604 - val_auc: 0.8480 - val_loss: 0.7622 - val_precision: 0.3148 - val_recall: 0.8716\n",
      "Epoch 10/25\n",
      "\u001b[1m1205/1205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7464 - auc: 0.9664 - loss: 0.9486 - precision: 0.3986 - recall: 0.9943 - val_accuracy: 0.6824 - val_auc: 0.8497 - val_loss: 0.7563 - val_precision: 0.3288 - val_recall: 0.8592\n",
      "Epoch 11/25\n",
      "\u001b[1m1205/1205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7821 - auc: 0.9718 - loss: 0.8565 - precision: 0.4351 - recall: 0.9956 - val_accuracy: 0.7072 - val_auc: 0.8476 - val_loss: 0.7290 - val_precision: 0.3456 - val_recall: 0.8355\n",
      "Epoch 12/25\n",
      "\u001b[1m1205/1205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8106 - auc: 0.9766 - loss: 0.7759 - precision: 0.4687 - recall: 0.9957 - val_accuracy: 0.7234 - val_auc: 0.8460 - val_loss: 0.7198 - val_precision: 0.3578 - val_recall: 0.8176\n",
      "Epoch 13/25\n",
      "\u001b[1m1205/1205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8308 - auc: 0.9792 - loss: 0.7254 - precision: 0.4980 - recall: 0.9948 - val_accuracy: 0.7414 - val_auc: 0.8411 - val_loss: 0.6935 - val_precision: 0.3722 - val_recall: 0.7899\n",
      "Epoch 14/25\n",
      "\u001b[1m1205/1205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8451 - auc: 0.9815 - loss: 0.6763 - precision: 0.5184 - recall: 0.9952 - val_accuracy: 0.7445 - val_auc: 0.8421 - val_loss: 0.7103 - val_precision: 0.3753 - val_recall: 0.7888\n",
      "Epoch 15/25\n",
      "\u001b[1m1205/1205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8585 - auc: 0.9834 - loss: 0.6395 - precision: 0.5422 - recall: 0.9947 - val_accuracy: 0.7510 - val_auc: 0.8411 - val_loss: 0.7125 - val_precision: 0.3811 - val_recall: 0.7778\n",
      "Epoch 16/25\n",
      "\u001b[1m1205/1205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8697 - auc: 0.9848 - loss: 0.6067 - precision: 0.5614 - recall: 0.9954 - val_accuracy: 0.7566 - val_auc: 0.8391 - val_loss: 0.7125 - val_precision: 0.3862 - val_recall: 0.7674\n",
      "Epoch 17/25\n",
      "\u001b[1m1205/1205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8764 - auc: 0.9859 - loss: 0.5843 - precision: 0.5750 - recall: 0.9949 - val_accuracy: 0.7559 - val_auc: 0.8388 - val_loss: 0.7357 - val_precision: 0.3856 - val_recall: 0.7682\n",
      "Epoch 18/25\n",
      "\u001b[1m1205/1205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8842 - auc: 0.9869 - loss: 0.5615 - precision: 0.5915 - recall: 0.9949 - val_accuracy: 0.7676 - val_auc: 0.8359 - val_loss: 0.7144 - val_precision: 0.3974 - val_recall: 0.7481\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "num_users = len(user_id_mapping)\n",
    "\n",
    "def build_model(hp):\n",
    "    # Hyperparameters to tune, the embedding dimensions, number of layers in MLP, dropout rate and learning rate\n",
    "    gmf_embedding_dim = hp.Choice('gmf_embedding_dim', values=[12, 18, 24, 30]) # Too big = missing contextual data, too small = loosing valuable data\n",
    "    mlp_embedding_dim = hp.Choice('mlp_embedding_dim', values=[32, 40, 48, 56]) # Too big = missing contextual data, too small = loosing valuable data\n",
    "    num_layers = hp.Int('num_layers', min_value=1, max_value=3) # Input layer + 1-3 hidden layers + output layer\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.2, step=0.05) # To prevent overfitting, randomly turn of neurons\n",
    "    learning_rate = hp.Choice('learning_rate', values=[5e-3, 1e-3, 5e-4, 1e-4]) # Find optimal learning rate\n",
    "    \n",
    "    # Inputs (a user, an outfit with its metadata)\n",
    "    user_input = Input(shape=(1,), name='user_input')\n",
    "    item_input = Input(shape=(1,), name='item_input')\n",
    "    item_metadata_input = Input(shape=(item_feature_dim,), name='item_metadata_input')\n",
    "    \n",
    "    # Embeddings\n",
    "    user_embedding_gmf = Embedding(num_users, gmf_embedding_dim, name='user_embedding_dim_gmf', embeddings_regularizer=l2(1e-5))(user_input)\n",
    "    item_embedding_gmf = Embedding(num_items, gmf_embedding_dim, name='item_embedding_gmf', embeddings_regularizer=l2(1e-5))(item_input)\n",
    "    user_embedding_mlp = Embedding(num_users, mlp_embedding_dim, name='user_embedding_dim_mlp', embeddings_regularizer=l2(1e-4))(user_input)\n",
    "    item_embedding_mlp = Embedding(num_items, mlp_embedding_dim, name='item_embedding_mlp', embeddings_regularizer=l2(1e-4))(item_input)\n",
    "\n",
    "    \n",
    "    # Flatten embeddings\n",
    "    user_latent_gmf = Flatten()(user_embedding_gmf)\n",
    "    item_latent_gmf = Flatten()(item_embedding_gmf)\n",
    "    user_latent_mlp = Flatten()(user_embedding_mlp)\n",
    "    item_latent_mlp = Flatten()(item_embedding_mlp)\n",
    "    \n",
    "    # Integrate metadata into MLP part\n",
    "    item_latent_mlp = Concatenate()([item_latent_mlp, item_metadata_input]) #With metadata\n",
    "    #item_latent_mlp = item_latent_mlp #Without Metadata\n",
    "\n",
    "    \n",
    "    # GMF and MLP interaction layers\n",
    "\n",
    "    #mf_vector = Dot(axes=1)([user_latent_gmf, item_latent_gmf]) # Dot product is a viable option to Multiply which does element wise product\n",
    "\n",
    "    # GMF\n",
    "    gmf_vector = Multiply()([user_latent_gmf, item_latent_gmf])\n",
    "    gmf_vector = Dropout(dropout_rate)(gmf_vector)  # Add dropout to GMF component\n",
    "    #gmf_vector = Activation('sigmoid')(gmf_vector) # Paper wanter us to use sigmoid activation function for the generalized mf vector. However we got better results not using it. \n",
    "\n",
    "    # Concatenate user and item for mlp\n",
    "    mlp_vector = Concatenate()([user_latent_mlp, item_latent_mlp])\n",
    "    \n",
    "    # MLP layers\n",
    "    for i in range(num_layers):\n",
    "        units = hp.Int(f'units_{i}', min_value=4, max_value=12, step=2) # Random, tries to find optimal in HPT.\n",
    "        mlp_vector = Dense(units, activation='relu')(mlp_vector) # ReLu worked best\n",
    "        mlp_vector = Dropout(dropout_rate)(mlp_vector)  # Dropout after each dense layer\n",
    "    \n",
    "    # Combine GMF and MLP parts\n",
    "    neu_vector = Concatenate()([gmf_vector, mlp_vector])\n",
    "    neu_vector = Dropout(dropout_rate)(neu_vector)  # Additional dropout before the final dense layer\n",
    "    \n",
    "    # Output layer using sigmoid to scale the results from 0-1\n",
    "    output = Dense(1, activation='sigmoid', name='output')(neu_vector)\n",
    "    \n",
    "    # Compile model\n",
    "    model = Model(inputs=[user_input, item_input, item_metadata_input], outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy', #\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.AUC(name='auc'),\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define the tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective=kt.Objective('val_auc', direction='max'),\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_dir',\n",
    "    project_name='neumf_tuning_v2_v8' #'neumf_tuning_v2_v7 or v8' best one yet\n",
    ")\n",
    "\n",
    "# Prepare training and validation data\n",
    "train_inputs = [train_user, train_item, train_item_metadata]\n",
    "train_labels = train_label\n",
    "\n",
    "val_inputs = [test_user, test_item, test_item_metadata]\n",
    "val_labels = test_label\n",
    "\n",
    "# Assign class weights because of imbalanced dataset\n",
    "positive_class_weight_multiplier = number_negative_per_positive*1.5  # Adjust this as needed\n",
    "class_weights = {0: 1.0, 1: positive_class_weight_multiplier * len(train_label) / (2 * sum(train_label))}\n",
    "\n",
    "# Start the hyperparameter search\n",
    "tuner.search(\n",
    "    train_inputs,\n",
    "    train_labels,\n",
    "    epochs=10,\n",
    "    validation_data=(val_inputs, val_labels),\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
    ")\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the model with the optimal hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_inputs,\n",
    "    train_labels,\n",
    "    batch_size=256,\n",
    "    epochs=25,\n",
    "    validation_data=(val_inputs, val_labels),\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a01d04-fa30-464d-9046-7f8245be23f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(best_hps.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb321d1c-7d63-48ff-82e1-24ccb43ebc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_auc_score\n",
    "\n",
    "# Get predicted probabilities for the test set\n",
    "y_probs = model.predict([test_user, test_item, test_item_metadata])\n",
    "# Compute precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(test_label, y_probs)\n",
    "# Find threshold with the best F1 score\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_pred_probs = model.predict(val_inputs, batch_size=1024).flatten()\n",
    "test_pred = (test_pred_probs >= optimal_threshold).astype(int)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(val_labels, test_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(f\"\"\"\n",
    "Confusion Matrix (Test Set):\n",
    "[[TN FP]\n",
    " [FN TP]]\n",
    "{cm}\n",
    "\n",
    "True Negatives: {tn}\n",
    "False Positives: {fp}\n",
    "False Negatives: {fn}\n",
    "True Positives: {tp}\n",
    "\"\"\")\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(val_labels, test_pred, digits=4))\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(val_labels, test_pred_probs)\n",
    "auc_score = roc_auc_score(val_labels, test_pred_probs)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = {:.2f})'.format(auc_score))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.savefig(\"ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cb7613-e6e6-49d0-8c3c-a87b6b7b2006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Calculate the AUC score\n",
    "auc_score = roc_auc_score(val_labels, test_pred_probs)\n",
    "\n",
    "# Print the AUC score\n",
    "print(f\"AUC Score: {auc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7a962e-2f5e-4e11-8e94-c153274bec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(history.history['accuracy'], label='Train')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "plt.title('Neural MF Model Accuracy with metadata')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.savefig(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.title('Neural MF Model Loss with metadata')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.savefig(\"Model loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c458aa5-c138-4501-a448-92c8f22b1069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, \n",
    "            xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
    "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('CMatrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
